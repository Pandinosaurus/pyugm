{
 "metadata": {
  "name": "",
  "signature": "sha256:42333c6e66c3edd044e727ce2816f7559885335269d7286b6f68e2e493b9ccc3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# pyugm models\n",
      "\n",
      "This notebook shows how to specify a simple discrete undirected probabilistic graphical model and perform common operations like\n",
      "marginalisation and calibration."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction\n",
      "\n",
      "The aim of the package is to provide ways to quickly specify and test out undirected probabilistic graphical models. \n",
      "At the moment it is too slow to tackle even medium sized problems (like in vision),\n",
      "but I plan to move the main inference routines to Cython soon, which should should make the package more generally useful.\n",
      "\n",
      "I hope to incorporate some the nice features of my two favourite machine learning packages, `sklearn` and `PyMC`.\n",
      "\n",
      "- `sklearn` provides a uniform interface to different models and many of the common preprocessing steps. \n",
      "pyugm models should therefore have a similar interface and the necessary helpers to easily apply a model to actual data.\n",
      "A drawback of sklearn, IMHO, is that almost none of the models are Bayesian (even for the Bayesian ridge regression model it is\n",
      "difficult to get the posterior or the predictive distribution).\n",
      "\n",
      "- `PyMC`, on the other hand, is fully Bayesian and a wonderful tool for many proplems. I find, however, that it is\n",
      "sometimes difficult to specify models with many types of observed variables - it seems to me to be more aimed at models with output\n",
      "variables of a single type.\n",
      "\n",
      "See http://www.cs.ubc.ca/~murphyk/Software/bnsoft.html."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Specification\n",
      "### Factors\n",
      "\n",
      "The main building blocks. At the moment discrete factors are implemented with numpy ndarrays. \n",
      "Note that almost all the operations on factors\n",
      "change the state of the original factor (except marginalisation which returns a new factor)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from pyugm.factor import DiscreteFactor\n",
      "\n",
      "# Specify the potential table\n",
      "factor_data = np.array([[1, 2], [2, 1]])\n",
      "# The variable names (\"1\" and \"2\") and cardinalities (2 and 2).\n",
      "variables_names_and_cardinalities = [(1, 2), (2, 2)]\n",
      "# Construct the factor\n",
      "factor = DiscreteFactor(variables_names_and_cardinalities, data=factor_data)\n",
      "print factor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1, 2}\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "factor.data  # The potential table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "array([[ 1.,  2.],\n",
        "       [ 2.,  1.]])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Marginalise out all the variables that is not named \"1\". (i.e. marginalise out variable \"2\")\n",
      "marg = factor.marginalize([1])\n",
      "print marg\n",
      "print marg.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1}\n",
        "[ 3.  3.]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reduce the original factor by observing variable \"1\" taking on the value 0. [TODO: implement efficient factor reduction]\n",
      "# Evidence is set by a dictionary where the key is a variable name and the value its observed value.\n",
      "factor.set_evidence({1: 0})\n",
      "print factor\n",
      "print factor.data "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1, 2}\n",
        "[[ 1.  2.]\n",
        " [ 0.  0.]]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Models\n",
      "\n",
      "Models are collections of factors. The model automatically builds a cluster graph by greedily adding the factor that have the\n",
      "largest separator set with a factor already in the graph. By using this scheme you will often end up with a tree. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyugm.model import Model\n",
      "\n",
      "factor1 = DiscreteFactor([(1, 2), (2, 2)], data=np.array([[1, 2], [2, 1]]))\n",
      "factor2 = DiscreteFactor([(2, 2), ('variable3', 3)],  # Variable names can also be strings\n",
      "                         data=np.array([[0, 0.2, 0.3], [0.1, 0.5, 0.3]]))  # Cardinalities of 2 and 3 means the factor table must be 2x3\n",
      "# [TODO: cardinalities can be inferred from data shape when provided]\n",
      "factor3 = DiscreteFactor([('variable3', 3), (4, 2)], data=np.array([[0, 1], [1, 2], [0.5, 0]]))\n",
      "model = Model([factor1, factor2, factor3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.edges  # returns a set up tuples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{(F{variable3, 4}, F{2, variable3}), (F{1, 2}, F{2, variable3})}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the graph:\n",
      "> factor1 -- factor2 -- factor3\n",
      "\n",
      "has been built."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Reduce all factors containing the observed variables\n",
      "model.set_evidence({'variable3': 2, 2: 0})\n",
      "print factor1\n",
      "print factor1.data\n",
      "print factor2\n",
      "print factor2.data\n",
      "print factor3\n",
      "print factor3.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1, 2}\n",
        "[[ 1.  0.]\n",
        " [ 2.  0.]]\n",
        "F{2, variable3}\n",
        "[[ 0.   0.   0.3]\n",
        " [ 0.   0.   0. ]]\n",
        "F{variable3, 4}\n",
        "[[ 0.   0. ]\n",
        " [ 0.   0. ]\n",
        " [ 0.5  0. ]]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Inference\n",
      "\n",
      "Run loopy belief propagation on a model. - Actually it is not the message passing version of belief propagation but the \n",
      "factor update algorithm."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyugm.model import Model\n",
      "from pyugm.infer import LoopyBeliefUpdateInference\n",
      "\n",
      "factor1 = DiscreteFactor([(1, 2), (2, 2)], data=np.array([[1, 2], [2, 1]]))\n",
      "factor2 = DiscreteFactor([(2, 2), ('variable3', 3)],   data=np.array([[0, 0.2, 0.3], [0.1, 0.5, 0.3]]))  \n",
      "factor3 = DiscreteFactor([('variable3', 3), (4, 2)], data=np.array([[0, 1], [1, 2], [0.5, 0]]))\n",
      "\n",
      "model = Model([factor1, factor2, factor3])\n",
      "inferrer = LoopyBeliefUpdateInference(model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_ = inferrer.calibrate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calibrated marginals\n",
      "print model.get_marginals(1)[0], model.get_marginals(1)[0].data\n",
      "print model.get_marginals(2)[0], model.get_marginals(2)[0].data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1} [ 2.72222222  0.        ]\n",
        "F{2} [ 0.          2.72222222]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalizing factor\n",
      "print model.get_marginals(1)[0], model.get_marginals(1)[0].data.sum()\n",
      "print model.get_marginals(2)[0], model.get_marginals(2)[0].data.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1} 2.72222222222\n",
        "F{2} 2.72222222222\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusion\n",
      "\n",
      "Although many improvements are necessary I hope this gives a glimpse of what I'm aiming at. I'll discuss parameter learning\n",
      "and different update orderings in another notebook."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}