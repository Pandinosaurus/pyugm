{
 "metadata": {
  "name": "",
  "signature": "sha256:7dc7d72af059ed9e08191a9e90abaa244e98a3ad8884982e3e6cd77e4777a10b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# `pyugm` models\n",
      "\n",
      "This notebook shows how to specify a simple discrete undirected probabilistic graphical model and perform common operations like\n",
      "marginalisation and calibration."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction\n",
      "\n",
      "The aim of the package is to provide ways to quickly specify and test out undirected probabilistic graphical models. \n",
      "At the moment it is too slow to tackle even medium sized problems (like in vision),\n",
      "but I plan to move the main inference routines to `Cython` soon, which should should make the package more generally usable.\n",
      "\n",
      "I hope to incorporate some the nice features of my two favourite machine learning packages, `sklearn` and `PyMC`.\n",
      "\n",
      "- `sklearn` provides a uniform interface to different models and many of the common preprocessing steps. \n",
      "`pyugm` models should therefore have a similar interface and the necessary helpers to easily apply a model to actual data.\n",
      "A drawback of `sklearn`, IMHO, is that almost none of the models are Bayesian (even for the Bayesian ridge regression model it is\n",
      "difficult to get the posterior or the predictive distribution).\n",
      "\n",
      "- `PyMC`, on the other hand, is fully Bayesian and a wonderful tool for many proplems. I find, however, that it is\n",
      "sometimes difficult to specify models with many types of observed variables - it seems to me to be more aimed at models with output\n",
      "variables of a single type.\n",
      "\n",
      "See http://www.cs.ubc.ca/~murphyk/Software/bnsoft.html."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model specification\n",
      "### Factors\n",
      "\n",
      "The main building blocks. \n",
      "\n",
      "At the moment discrete factors are implemented with numpy ndarrays. \n",
      "Note that factors should be immutable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from pyugm.factor import DiscreteFactor\n",
      "\n",
      "# Specify the potential table\n",
      "factor_data = np.array([[1, 2], [2, 1]])\n",
      "# The variable names (\"1\" and \"2\") and cardinalities (2 and 2).\n",
      "variables_names_and_cardinalities = [(1, 2), (2, 2)]\n",
      "# Construct the factor\n",
      "factor = DiscreteFactor(variables_names_and_cardinalities, data=factor_data)\n",
      "print factor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1, 2}\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "factor.data  # The potential table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([[ 1.,  2.],\n",
        "       [ 2.,  1.]])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Marginalise out all the variables that is not named \"1\". (i.e. marginalise out variable \"2\")\n",
      "marg = factor.marginalize([1])\n",
      "print marg\n",
      "print marg.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1}\n",
        "[ 3.  3.]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Beliefs\n",
      "`Belief`s are mutable `Factor`s. They contain the current belief over the variables in the factor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyugm.factor import DiscreteBelief\n",
      "# Create a belief that is based on a factor\n",
      "belief = DiscreteBelief(factor)\n",
      "# Reduce the original factor by observing variable \"1\" taking on the value 0. [TODO: implement efficient factor reduction]\n",
      "# Evidence is set by a dictionary where the key is a variable name and the value its observed value.\n",
      "belief.set_evidence({1: 0})\n",
      "print belief\n",
      "print belief.data "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1, 2}\n",
        "[[ 1.  2.]\n",
        " [ 0.  0.]]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Models\n",
      "\n",
      "Models are collections of factors. The model automatically builds a cluster graph by greedily adding the factor that have the\n",
      "largest separator set with a factor already in the graph. By using this scheme you will often end up with a tree. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyugm.model import Model\n",
      "\n",
      "factor1 = DiscreteFactor([(1, 2), (2, 2)], data=np.array([[1, 2], [2, 1]]))\n",
      "factor2 = DiscreteFactor([(2, 2), ('variable3', 3)],  # Variable names can also be strings\n",
      "                         data=np.array([[0, 0.2, 0.3], [0.1, 0.5, 0.3]]))  # Cardinalities of 2 and 3 means the factor table must be 2x3\n",
      "# [TODO: cardinalities can be inferred from data shape when provided]\n",
      "factor3 = DiscreteFactor([('variable3', 3), (4, 2)], data=np.array([[0, 1], [1, 2], [0.5, 0]]))\n",
      "model = Model([factor1, factor2, factor3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.edges  # returns a set of tuples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "{(F{1, 2}, F{2, variable3}), (F{2, variable3}, F{variable3, 4})}"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The graph:\n",
      "> factor1 -- factor2 -- factor3\n",
      "\n",
      "has been built."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Inference\n",
      "Models contain immutable `factor`s, while `Inference` objects contain `belief`s. `Inference` objects contain the `calibrate`\n",
      "method to calibrate the `belief`s."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from pyugm.model import Model\n",
      "from pyugm.infer_message import LoopyBeliefUpdateInference"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run loopy belief propagation on a new model. - Actually it is not the message passing version of belief propagation but the \n",
      "belief update algorithm."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "factor1 = DiscreteFactor([(1, 2), (2, 2)], data=np.array([[1, 2], [2, 1]]))\n",
      "factor2 = DiscreteFactor([(2, 2), ('variable3', 3)], data=np.array([[0, 0.2, 0.3], [0.1, 0.5, 0.3]]))  \n",
      "factor3 = DiscreteFactor([('variable3', 3), (4, 2)], data=np.array([[0, 1], [1, 2], [0.5, 0.1]]))\n",
      "\n",
      "model = Model([factor1, factor2, factor3])\n",
      "inferrer = LoopyBeliefUpdateInference(model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inferrer.calibrate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 160,
       "text": [
        "<pyugm.infer_message.LoopyBeliefUpdateInference at 0x10771b2d0>"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calibrated marginals\n",
      "print inferrer.get_marginals(1)[0], inferrer.get_marginals(1)[0].data\n",
      "print inferrer.get_marginals(2)[0], inferrer.get_marginals(2)[0].data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1} [ 0.56510417  0.43489583]\n",
        "F{2} [ 0.3046875  0.6953125]\n"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Natural logarithm of the normalizing factor\n",
      "print inferrer.partition_approximation()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.03861954626\n"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What are the marginals when we observe `variable3 = 1` ?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inferrer.calibrate(evidence={'variable3': 1})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 163,
       "text": [
        "<pyugm.infer_message.LoopyBeliefUpdateInference at 0x10771b2d0>"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calibrated marginals\n",
      "print inferrer.get_marginals(1)[0], inferrer.get_marginals(1)[0].data\n",
      "print inferrer.get_marginals(2)[0], inferrer.get_marginals(2)[0].data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F{1} [ 0.50759607  0.49240393]\n",
        "F{2} [ 0.4772118  0.5227882]\n"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Natural logarithm of the normalizing factor\n",
      "print inferrer.partition_approximation()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.75892086647\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusion\n",
      "\n",
      "Although many improvements are necessary I hope this gives a glimpse of what I'm aiming at. I'll discuss parameter learning\n",
      "and different update orderings in another notebook."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}